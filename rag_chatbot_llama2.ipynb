{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1e6b5a-f475-41ee-a7f3-3824143b74d4",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ded36d-5c34-4790-bc2b-767da3556046",
   "metadata": {},
   "source": [
    "**What's RAG :-** RAG is an AI framework that allows a Generative AI model(LLM) to access external information not included in its training data or model parameters to enhance its responses to prompts.\n",
    "\n",
    "**Why it's important? :-** LLMs are prone to hallucination i.e. they present false information when they don't have answer. Also LLMs often present outdated information or response from non-authoritative sources. RAG is one approach to solving such challenges.\n",
    "\n",
    "**Let's see how to use with *Open Source* Models and Frameworks...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bcc996-59a6-40eb-a7e5-85f0704ac91b",
   "metadata": {},
   "source": [
    "> #### Importing libraries and frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3267f1-a210-48ce-a3a5-d4224fc7d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "from langchain.llms import LlamaCpp\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be3222-4141-4e44-bac8-635ef58ffa2f",
   "metadata": {},
   "source": [
    "> #### Creating Embedding with open source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b467df95-4840-496a-9230-5c39025451f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\core\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size':32}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a765680-3857-43dc-ab0f-da9698226c89",
   "metadata": {},
   "source": [
    "> #### Demo of how the downloaded embedding work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e99efd9-3382-46c8-a08d-bb923e90395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 doc embeddings, each with a dimensionality of 384.\n"
     ]
    }
   ],
   "source": [
    "doc = ['first line of document', 'another line']\n",
    "\n",
    "embeddings = embed_model.embed_documents(doc)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fcc9f-79ca-4227-900f-642ccc652806",
   "metadata": {},
   "source": [
    "> #### Loading Data\n",
    "\n",
    "Since I like WWE, so am picking some latest WWE data which is ofcourse not available on most LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c547cc-6327-4e94-b098-62471b9248d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cagesideseats.com/wwe/2024/1/5/24026239/wwe-smackdown-results-live-blog-jan-5-2024-new-years-revolution-orton-knight-styles-mystery-partner'\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "document = loader.load()\n",
    "\n",
    "# loader = TextLoader(\"chat_data.txt\")\n",
    "# documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c0158aa-e9a4-4e9d-8bf2-7e5634ebbe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Skip to main content\\n\\n\\n\\nclock\\nmenu\\nmore-arrow\\nno\\nyes\\nmobile\\n\\n\\n\\n\\n\\n\\n\\n\\nCageside Seats homepage', metadata={'source': 'https://www.cagesideseats.com/wwe/2024/1/5/24026239/wwe-smackdown-results-live-blog-jan-5-2024-new-years-revolution-orton-knight-styles-mystery-partner', 'title': 'WWE SmackDown results, live blog (Jan. 5, 2024): New Year’s Revolution - Cageside Seats', 'description': 'Follow along with this week’s New Year’s Revolution episode of SmackDown on FOX, featuring Randy Orton vs. LA Knight vs. AJ Styles, appearances from Roman Reigns and Logan Paul, BUTCH has a mystery partner, IYO SKY defends the women’s championship, Kevin Owens and Santos Escobar fight in a tournament final, and more!', 'language': 'en'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d818d-8ac0-48ba-8aef-45c04334ece0",
   "metadata": {},
   "source": [
    "> #### Building the Vector Index using ChromaDB\n",
    "\n",
    "We now need to use the embedding pipeline to build our embeddings and store them in ChromaDB vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a4b335-14a2-44ad-b50a-21bf5c1182d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents=texts, embedding=embed_model)\n",
    "\n",
    "retriever = db.as_retriever(search_type = \"similarity\", search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b85c15-fd1d-422f-8977-bea44c72cb01",
   "metadata": {},
   "source": [
    "> #### Loading Large Language Model\n",
    "\n",
    "Here am going to pick Llama2 13B Chat quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52b24beb-bb6b-4af1-bf6e-62e29d305d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
    "\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename, local_dir='E:\\coding\\PycharmProjects\\medical-chatbot\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddfff680-ae7e-4292-87cc-f0e83acd9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=model_path,\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1caa99-0027-4861-8580-e50e3264a1ce",
   "metadata": {},
   "source": [
    "> #### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd16b93f-4dc0-431b-ae48-a9f3aeea825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "You are an assistant to a human, powered by a large language model.\n",
    "As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. \n",
    "You have access to some personalized information provided by the human in the Context section below. \n",
    "\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Return the helpful answer and nothing else. Make sure to double check your answer.\n",
    "Don't make up the answer.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e18be-ed48-412e-b827-a8aeaa495b55",
   "metadata": {},
   "source": [
    "> #### Let's Chat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "804fed3b-0c45-45d9-a16a-6a70dea60749",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline  = RetrievalQA.from_chain_type(\n",
    "    llm=llm,                                      #LLM Model\n",
    "    chain_type=\"stuff\",                           #Type of chain\n",
    "    retriever=retriever,                          #Vector database \n",
    "    return_source_documents=False,                 #Want to know from where answer came?\n",
    "    memory=ConversationBufferMemory(),            #LangChain memory\n",
    "    chain_type_kwargs=chain_type_kwargs           #Prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed90adce-beec-41ab-ad33-a61f42eeb83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Prompt: How Kevin Owens won the match?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response : Kevin Owens won the match with a pinfall victory over his opponent, using his patented pop-up powerbomb maneuver.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Prompt: Who was the winner - IYO SKY (c) vs. “Michin” Mia Yim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response : The winner of the match between IYO SKY (c) and “Michin” Mia Yim was IYO SKY (c).\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Prompt: exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    result=rag_pipeline({\"query\": user_input})\n",
    "    print(f\"Response : {result['result']}\\n\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b350b-694b-4ba6-a0ad-a9b4f720bcee",
   "metadata": {},
   "source": [
    "> #### This is how you can talk to your documents or create chatbot based on an intensive QnA data of your business.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bbfea-bbef-4dbf-87c7-1e778c2cff09",
   "metadata": {},
   "source": [
    "By - [Himanshu Goswami](https://www.linkedin.com/in/himgos/) 🧑‍💻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47dbf2-5ee3-4e5f-9449-b1f152d2a3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
